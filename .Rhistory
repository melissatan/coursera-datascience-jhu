predrf <- predict(modrf, testing)
confusionMatrix(predrf, testing$diagnosis)$overall
predgbm <- predict(modgbm, testing)
confusionMatrix(predgbm, testing$diagnosis)$overall
predlda <- predict(modlda, testing)
confusionMatrix(predlda, testing$diagnosis)$overall
predstack <- data.frame(predrf, predgbm, predlda, diagnosis=testing$diagnosis)
combimod <-train(diagnosis~., method="rf", data=predstack)
combipred <- predict(combimod, predstack)
confusionMatrix(combipred, testing$diagnosis)$overall
set.seed(62433)
modrf <- train(diagnosis~., data=training, method="rf")
modgbm <- train(diagnosis~., data=training, method="gbm", verbose=FALSE)
modlda <- train(diagnosis~., data=training, method="lda")
predrf <- predict(modrf, testing)
confusionMatrix(predrf, testing$diagnosis)$overall
predgbm <- predict(modgbm, testing)
confusionMatrix(predgbm, testing$diagnosis)$overall
predlda <- predict(modlda, testing)
confusionMatrix(predlda, testing$diagnosis)$overall
predstack <- data.frame(predrf, predgbm, predlda, diagnosis=testing$diagnosis)
combimod <-train(diagnosis~., method="rf", data=predstack)
combipred <- predict(combimod, predstack)
confusionMatrix(combipred, testing$diagnosis)$overall
predstack <- data.frame(predrf, predgbm, predlda, diagnosis=testing$diagnosis)
combimod <-train(diagnosis~., method="rf", data=predstack)
combipred <- predict(combimod, predstack)
confusionMatrix(combipred, testing$diagnosis)$overall
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain, ]
testing = adData[-inTrain, ]
# Set the seed to 62433 and predict diagnosis with all the other variables using a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis ("lda") model.
set.seed(62433)
modrf <- train(diagnosis~., data=training, method="rf")
modgbm <- train(diagnosis~., data=training, method="gbm", verbose=FALSE)
modlda <- train(diagnosis~., data=training, method="lda")
predrf <- predict(modrf, testing)
confusionMatrix(predrf, testing$diagnosis)$overall  # rf acc: 0.805
predgbm <- predict(modgbm, testing)
confusionMatrix(predgbm, testing$diagnosis)$overall  # gbm acc: 0.793
predlda <- predict(modlda, testing)
confusionMatrix(predlda, testing$diagnosis)$overall  # lda acc: 0.768
# Stack the predictions together using random forests ("rf"). What is the resulting accuracy on the test set? Is it better or worse than each of the individual predictions?
predstack <- data.frame(predrf, predgbm, predlda, diagnosis=testing$diagnosis)
combimod <-train(diagnosis~., method="rf", data=predstack)
combipred <- predict(combimod, predstack)
confusionMatrix(combipred, testing$diagnosis)$overall
install.packages("elasticnet")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
modfit <- train(CompressiveStrength~., method="lasso", data=training)
plot(modfit)
class(modfit)
library(elasticnet)
enet(x=subset(training, select=-c(CompressiveStrength)),
y=training$CompressiveStrength,
lambda = 0, trace=TRUE)
predictors <- subset(training, select=-c(CompressiveStrength))
enet(x=predictors, y=training$CompressiveStrength,
lambda = 0, trace=TRUE)
enet(x=predictors, y=training$CompressiveStrength, lambda = 0)
names(training)
names(predictors)
modlasso <- enet(x=predictors, y=training$CompressiveStrength, lambda = 0, trace=TRUE)
predictorM <- as.matrix(subset(training, select=-c(CompressiveStrength)))
modlasso <- enet(x=predictorM, y=as.numeric(training$CompressiveStrength), lambda = 0, trace=TRUE)
plot(modlasso)
set.seed(233)
predictorM <- as.matrix(subset(training, select=-c(CompressiveStrength)))
modlasso <- enet(x=predictorM, y=as.numeric(training$CompressiveStrength),
lambda = 0, trace=TRUE)
plot(modlasso)
plot(modlasso, xvar="penalty")
plot(modlasso, xvar="penalty", use.color=TRUE)
plot(modlasso, xvar="penalty", use.color=TRUE, las=1)
par(las=1)
plot(modlasso, xvar="penalty", use.color=TRUE)
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip")
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "./dataset.zip")
setwd("./capstone")
if (!file.exists("./final")) {
fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(fileUrl, destfile = "./swiftkey.zip")
unzip("./swiftkey.zip")
}
blogs.txt <- "./final/en_US/en_US.blogs.txt"
news.txt <- "./final/en_US/en_US.news.txt"
twitter.txt <- "./final/en_US/en_US.twitter.txt"
blogs.header <- readLines(blogs.txt, n=1)
blogs.header
readLines(blogs.txt, n=3)
readLines(news.txt, n=1)
readLines(news.txt, n=3)
readLines(twitter.txt, n=3)
tools::md5sum("./swiftkey.zip")
readLines(twitter.txt, n=2:3)
con <- file(twitter.txt)
open(con)
readLines(con, n=1)
readLines(con, n=1)
readLines(con, n=1)
readLines(con, n=1)
readLines(con, n=1)
readLines(con, n=1)
readLines(con, n=1)
readLines(con, n=1)
close(con)
test <- readLines(twitter.txt, n=1)
test
strsplit(test, split=" ")
test.split <- strsplit(test, split=" ")
test.split[1]
test.split[[1]]
test.split[[1]][1]
unlist(test.split)
unlisted <- unlist(test.split)
test.split
unlisted
con <- file(twitter.txt)
open(con)
twitter.list <- list()
i <- 0
while (length(oneLine <- readLines(con, n=1, warn=FALSE)) > 0) {
twitter.list[[i]] <- unlist(strsplit(oneLine, split=" "))
i <- i + 1
}
close(con)
con <- file(twitter.txt)
open(con)
twitter.list <- list()
i <- 0
oneLine <- readLines(con, n=1, warn=FALSE)
oneLine
length(oneLine)
twitter.list[1]
twitter.list[2]
twitter.list[[1]]
twitter.list[1] <- "a"
twitter.list[2] <- "b"
twitter.list
twitter.list[2] <- c("b","c","d")
twitter.list
twitter.list[[2]] <- c("b","c","d")
twitter.list
strsplit(oneLine, split=" ")
unlist(strsplit(oneLine, split=" "))
twitter.list[[1]] <- unlist(strsplit(oneLine, split=" "))
twitter.list
nextLine <- readLine(con, n=1)
nextLine <- readLines(con, n=1)
length(nextLine)
nextLine
twitter.list[[2]] <- unlist(strsplit(nextLine, split=" "))
twitter.list
close(con)
con <- file(twitter.txt)
open(con)
twitter.list <- list()
i <- 0
while (length(oneLine <- readLines(con, n=1)) > 0) {
twitter.list[[i]] <- unlist(strsplit(oneLine, split=" "))
i <- i + 1
}
while (length(oneLine <- readLines(con, n=1)) > 0) {
splitLine <- unlist(strsplit(oneLine, split=" "))
twitter.list[[i]] <- splitLine
i <- i + 1
}
splitLine
i
close(con)
con <- file(twitter.txt)
open(con)
twitter.list <- list()
i <- 0
while (length(oneLine <- readLines(con, n=1)) > 0) {
i <- i + 1
splitLine <- unlist(strsplit(oneLine, split=" "))
twitter.list[[i]] <- splitLine
}
con <- file(twitter.txt, open="r")
open(con)
twitter.list <- list()
i <- 0
while (length(oneLine <- readLines(con, n=1, warn=FALSE)) > 0) {
i <- i + 1
# splitLine <- unlist(strsplit(oneLine, split=" "))
twitter.list[[i]] <- oneLine
}
install.packages("R.utils")
library(R.utils)
countLines(twitter.txt)
x <- "I said,hey ,'tisn't 8 fun?!nope..(ha)"
myRemovePunct <- function(x) {
# replace everything that is not alphanumeric, space, or hyphen with a space
gsub("[^[:alnum:][:space:]']", " ", x)
}
myRemovePunct(x)
x <- "I said,hey ,'tisn't #8@& $5%*\m/ fun?!nope..(ha)"
x <- "I said,hey ,'tisn't #8@& $5%*\\m/ fun?!nope..(ha)"
x
myRemovePunct(x)
myRemovePunct <- function(x) {
# replace everything that is not alphanumeric, space, or hyphen with a space
gsub("[^[:alnum:][:space:]'-]", " ", x)
}
x <- "I said,hey ,'tisn't -#8@& $5%*\\m/ f-un?!nope..(ha)"
myRemovePunct(x)
y <- "this is my link http://www.com yes"
removeURL <- function(x) {
gsub("http[[:alnum:]]", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeMostPunct <- function(x) {
# replace everything that isn't alphanumeric, space, apostrophe or hyphen with a space
gsub("[^[:alnum:][:space:]'-]", " ", x)
}
removeURL(removeMostPunct(y))
removeURL <- function(x) {
gsub("http.*?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeURL <- function(x) {
gsub("http://.*?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeURL <- function(x) {
gsub("http://[:alnum:]+*?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeURL <- function(x) {
gsub("http://[:alnum:]+?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
y
removeURL <- function(x) {
gsub("http.+?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeURL <- function(x) {
gsub("http.*", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
y <- "this is my link http://ww.com and http:/yes.org"
removeURL(y)
removeURL <- function(x) {
gsub("http.*?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeURL <- function(x) {
gsub("http[:/]*.*?", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
removeURL <- function(x) {
gsub("http[:/]*.*", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
y <- "this is my link http://ww.com and http:/yes.org and here"
removeURL(u)
removeURL(y)
removeURL <- function(x) {
gsub("http[:/]*.+ ", "", x)  # won't work with some links e.g. bit.ly
}
removeURL(y)
y
gsub("http.*? ", "", y)
x <- "this is my link http://ww.com and http:/yes.org"
gsub("http.*? ", "", x)
gsub("http.*?[ $]", "", x)
gsub("http.*?( |$)", "", x)
gsub("http.*?[ $]", "", y)
x
y
gsub("http.*?[ $]", "", x)
gsub("http.*?( |$)", "", x)
gsub("http.*?( |$)", "", y)
a <- this is my link http://ww.com, and http:/yes.org. and here
a <- "this is my link http://ww.com, and http:/yes.org. and here"
gsub("http.*?( |$)", "", a)
a <- "this is my link http://ww.com , and http:/yes.org). and here"
gsub("http.*?( |$)", "", a)
x
gsub("http.*?( |$)", "", x)
x <- "My sister-in-law came--today---and -I- am a-okay!"
gsub("--+", "", x)
gsub("--+", " ", x)
gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
x <- "#hi My sister-in-law came--today---and -I- am a-okay!@home* (~ha)?"
gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
handleDashApost <- function(x) {
x <- gsub("--+", " ", x)
gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
}
handleDashApost(x)
x
handleDashApost(removeMostPunct(x))
x <- "#hi My sister-in-law came--today---and -I- isn't 'twas am a-okay!@home* (~ha)?"
handleDashApost(removeMostPunct(x))
stripWhitespace(handleDashApost(removeMostPunct(x)))
library(tm)
stripWhitespace(handleDashApost(removeMostPunct(x)))
x <- "'sup' #hi My sister-in-law came--today---and -I- isn't 'twas am a-okay!@home* (~ha)?\"yo\""
stripWhitespace(handleDashApost(removeMostPunct(x)))
trimWhitespace <- function(x) {
gsub("^\\s+|\\s+$", "", x)
}
trimWhitespace(stripWhitespace(handleDashApost(removeMostPunct(x))))
blog.mini <- readLines("./blog.sample.txt")  # imports as character vector
news.mini <- readLines("./news.sample.txt")
twit.mini <- readLines("./twit.sample.txt")
setwd("dscapstone")
system("ls")
blog.mini <- readLines("./blog.sample.txt")  # imports as character vector
news.mini <- readLines("./news.sample.txt")
twit.mini <- readLines("./twit.sample.txt")
library(tm)
# build a corpus, specifying the vector source as a character vector
blog.corpus <- Corpus(VectorSource(blog.mini))
inspect(blog.corpus[20])
blog.corpus <- tm_map(blog.corpus, content_transformer(tolower))
inspect(blog.corpus[20])
blog.corpus <- tm_map(blog.corpus, content_transformer(removeNumbers))
inspect(blog.corpus[20])
x <- "1st and 4most my b2b wd40 is 24/7 3lb 4 kg"
gsub("[[:digit:]]"," ",x)
gsub("[[:digit:]]+"," ",x)
gsub("[[:digit:]].+"," ",x)
gsub("[[:digit:]].*?"," ",x)
gsub("\w\d\w", " ",x)
gsub(".*?[0-9]+.*?", " ",x)
x
gsub("[^\s]*[0-9]+[^\s]*", " ", x)
gsub("\\S+[0-9]+\\S+", " ", x)
gsub("\\S*[0-9]+\\S+", " ", x)
gsub("\\S*[0-9]+\\S*", " ", x)
gsub("\\S*[0-9]+\\S*", "", x)
x <- "1st, and .4most my b2b wd40 is 24/7 ! 3lb! 4 kg?"
gsub("\\S*[0-9]+\\S*", "", x)
inspect(blog.corpus[20])
blog.corpus <- Corpus(VectorSource(blog.mini))
blog.corpus <- tm_map(blog.corpus, content_transformer(tolower))
# remove URLs within string and at end of string
removeURL <- function(x) {
gsub("http.*?( |$)", "", x)  # won't work with shortened URLs e.g. bit.ly
}
blog.corpus <- tm_map(blog.corpus, content_transformer(removeURL))
inspect(blog.corpus[20])
myRemoveNumbers <- function(x) {
gsub("\\S*[0-9]+\\S*", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemoveNumbers))
inspect(blog.corpus[20])
# remove punctuation. custom, since removePunctuation() removes too much
myRemovePunctuation <- function(x) {
# replace everything that isn't alphanumeric, space, apostrophe or hyphen
gsub("[^[:alnum:][:space:]'-]", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemovePunctuation))
inspect(blog.corpus[20])
myDashApos <- function(x) {
x <- gsub("--+", " ", x)
gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myDashApos))
inspect(blog.corpus[20])
blog.corpus <- tm_map(blog.corpus, content_transformer(stripWhitespace))
# trim leading and trailing whitespace
trim <- function(x) {
gsub("^\\s+|\\s+$", "", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(trim))
inspect(blog.corpus[20])
inspect(blog.corpus[22])
x <- "I passed Michaelâ€™s five-year old sister"
gsub("[\u2018\u2019\u201A\u201B\u2032\u2035]", "'", x)
gsub("[\x82\x91\x92]", "'", x)
blog.corpus <- Corpus(VectorSource(blog.mini))
# convert text to lowercase
blog.corpus <- tm_map(blog.corpus, content_transformer(tolower))
# remove URLs within string and at end of string
removeURL <- function(x) {
gsub("http.*?( |$)", "", x)  # won't work with shortened URLs e.g. bit.ly
}
blog.corpus <- tm_map(blog.corpus, content_transformer(removeURL))
myRemoveNumbers <- function(x) {
gsub("\\S*[0-9]+\\S*", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemoveNumbers))
# remove punctuation. custom, since removePunctuation() removes too much
myRemovePunctuation <- function(x) {
# replace everything that isn't alphanumeric, space, apostrophe or hyphen
gsub("[^[:alnum:][:space:]'-]", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemovePunctuation))
myDashApos <- function(x) {
# replace smart single quotes and apostrophes with straight single quotes
x <- gsub("[\x82\x91\x92]", "'", x)  # ANSI version, not Unicode version
x <- gsub("--+", " ", x)
gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myDashApos))
# strip extra whitespace
blog.corpus <- tm_map(blog.corpus, content_transformer(stripWhitespace))
# trim leading and trailing whitespace
trim <- function(x) {
gsub("^\\s+|\\s+$", "", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(trim))
inspect(blog.corpus[20])
inspect(blog.corpus[22])
blog.corpus <- Corpus(VectorSource(blog.mini))
# convert text to lowercase
blog.corpus <- tm_map(blog.corpus, content_transformer(tolower))
# remove URLs within string and at end of string
removeURL <- function(x) {
gsub("http.*?( |$)", "", x)  # won't work with shortened URLs e.g. bit.ly
}
blog.corpus <- tm_map(blog.corpus, content_transformer(removeURL))
# remove any word starting with numbers
myRemoveNumbers <- function(x) {
gsub("\\S*[0-9]+\\S*", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemoveNumbers))
# convert smart single quotes, like â€™, to straight single quotes, like '
mySingleQuote <- function(x) {
gsub("[\x82\x91\x92]", "'", x)  # ANSI version, not Unicode version
}
blog.corpus <- tm_map(blog.corpus, content_transformer(mySingleQuote))
# remove punctuation. custom, since removePunctuation() removes too much
myRemovePunctuation <- function(x) {
# replace everything that isn't alphanumeric, space, apostrophe or hyphen
gsub("[^[:alnum:][:space:]'-]", " ", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myRemovePunctuation))
# deal with dashes and apostrophes within words
myDashApos <- function(x) {
x <- gsub("--+", " ", x)
gsub("(\\w['-]\\w)|[[:punct:]]", "\\1", x, perl=TRUE)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(myDashApos))
# strip extra whitespace
blog.corpus <- tm_map(blog.corpus, content_transformer(stripWhitespace))
# trim leading and trailing whitespace
trim <- function(x) {
gsub("^\\s+|\\s+$", "", x)
}
blog.corpus <- tm_map(blog.corpus, content_transformer(trim))
inspect(blog.corpus[22])
getAnywhere(removePunctuation)
getMethod("removePunctuation")
inspect(blog.corpus[22])
blog.tdm <- TermDocumentMatrix(blog.corpus)
inspect(blog.tdm[1:20, 1:5])
blog.tdm
i <- which(dimnames(blog.tdm)$Terms == "winter")
i <- which(dimnames(blog.tdm)$Terms == "winter")
i
inspect(blog.tdm[i+(0:5), 1:5])
findFreqTerms(x=blog.tdm, lowfreq=1000, highfreq=Inf)
findFreqTerms(x=blog.tdm, lowfreq=10000, highfreq=Inf)
dimnames(tdm)
dimnames(blog.tdm)
dimnames(blog.tdm)$Terms[1:3]
blog.tdm
dimnames(blog.tdm)$Terms[69257]
findFreqTerms(blog.tdm, highfreq=1)
findFreqTerms(blog.tdm, lowfreq=0, highfreq=1)[1:5]
findFreqTerms(blog.tdm, lowfreq=0, highfreq=1)[1000:1005]
nrow(findFreqTerms(blog.tdm, lowfreq=0, highfreq=1))
once <- findFreqTerms(blog.tdm, lowfreq=0, highfreq=1)
dim(once)
str(once)
class(once)
length(once)
sample(once,5)
blog.freq
blog.freq <- rowSums(as.matrix(blog.tdm))
bdf <- as.data.frame(blog.tdm)
bm <- as.matrix(blog.tdm)
blog.tdm
bm <- inspect(blog.tdm)
findFreqTerms(x=blog.tdm, lowfreq=10000, highfreq=Inf)
install.packages("slam")
library(slam)
blog.freq <- row_sums(blog.tdm, na.rm=TRUE)
str(blog.freq)
hist(blog.freq)
max(blog.freq)
head(blog.freq)
blog.tdm.denser <- removeSparseTerms(blog.tdm, 0.9)
hist(row_sums(blog.tdm.denser, na.rm=T))
length(stopwords("english"))
head(stopwords("english"))
"i" %in% stopwords("english")
"you" %in% stopwords("english")
"and" %in% stopwords("english")
"but" %in% stopwords("english")
stopwords("english")
findAssocs(blog.tdm, "winter", 0.5)
findAssocs(blog.tdm, "winter", 0.1)
findAssocs(blog.tdm, "cold", 0.1)
findAssocs(blog.tdm, "snow", 0.1)
install.packages("RWeka")
